# ğŸ§  Emotion Classifier â€“ Fine-tuning DistilBERT

Ce projet met en place un pipeline complet de **classification dâ€™Ã©motions** Ã  partir du modÃ¨le prÃ©-entraÃ®nÃ© `distilbert-base-uncased`, fine-tunÃ© sur le dataset **Emotion** de ğŸ¤— Hugging Face.

---

## ğŸ¯ Objectifs
- Utiliser un modÃ¨le prÃ©-entraÃ®nÃ© (`distilbert-base-uncased`)
- Fine-tuner sur le dataset *Emotion* (6 Ã©motions)
- Sauvegarder le modÃ¨le
---

## ğŸ§± Structure du projet
```
emotion-classifier/
â”‚
â”œâ”€â”€ emotion_classification.ipynb   # Notebook principal
â”œâ”€â”€ models/                        # ModÃ¨le fine-tunÃ© sauvegardÃ©
â”œâ”€â”€ requirements.txt               # DÃ©pendances Python
â””â”€â”€ README.md                      # Documentation du projet
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Cloner le dÃ©pÃ´t
```bash
git clone [https://github.com/<ton_nom_utilisateur>/emotion-classifier.git](https://github.com/NicolasStucky0/Emotion-Classifier-Fine-tuning-DistilBERT.git)
cd emotion-classifier
```

### 2ï¸âƒ£ CrÃ©er un environnement virtuel (recommandÃ©)
```bash
python -m venv venv
source venv/bin/activate   # ou venv\Scripts\activate sous Windows
```

### 3ï¸âƒ£ Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

---

## ğŸš€ Utilisation

### ğŸ§© ExÃ©cuter le notebook
Lance le notebook `emotion_classification.ipynb` (sous Jupyter ou Google Colab)  
pour fine-tuner le modÃ¨le sur le dataset Emotion.

Le modÃ¨le sera automatiquement sauvegardÃ© dans :
```
./models/emotion-distilbert/
```

---

## ğŸ”® AmÃ©liorations possibles
- Ajuster le nombre dâ€™Ã©poques ou le learning rate  
- Tester dâ€™autres modÃ¨les (`bert-base-uncased`, `roberta-base`, etc.)  
- Ajouter des mÃ©triques (F1, confusion matrix, etc.)  
- DÃ©ployer le modÃ¨le sur Hugging Face Hub
